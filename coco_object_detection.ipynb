{"cells":[{"cell_type":"markdown","metadata":{"id":"UIyF_g9_E5OL"},"source":["# Introduction and Loading Dataset\n","\n","This project attempts to implement and train a YOLO (You Only Look Once) Object Detection model on the COCO 2017 dataset.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"7NTLzG-QE8bt","executionInfo":{"status":"ok","timestamp":1717137806893,"user_tz":-420,"elapsed":366,"user":{"displayName":"Norman Võ","userId":"13500557006273414443"}}},"outputs":[],"source":["%matplotlib inline\n","from pycocotools.coco import COCO\n","import numpy as np\n","import skimage.io as io\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.colors as colors\n","import pylab\n","from zipfile import ZipFile, BadZipFile\n","import os\n","import pathlib\n","pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n","\n","from __future__ import division\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import cv2\n","import time\n","import pickle as pkl"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"QCDYpL1hYwaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZzFXERHWHjiJ"},"outputs":[],"source":["# downloading the datasets\n","\n","# !wget http://images.cocodataset.org/zips/train2017.zip -O coco_train2017.zip\n","# !wget http://images.cocodataset.org/zips/val2017.zip -O coco_val2017.zip\n","# !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip -O coco_ann2017.zip\n","\n","# ROOT = \"./coco\"\n","# IMAGES_PATH = str(ROOT + \"/images\")\n","# ANNOTATIONS_PATH = str(ROOT + \"/instances.json\")\n","\n","# dataset = datasets.CocoDetection(IMAGES_PATH, ANNOTATIONS_PATH)"]},{"cell_type":"code","source":["from google.colab import drive\n","# run once to mount drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"VSqs18w5irpY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move files from drive, run if files are not already in root folder\n","!cp \"/content/drive/MyDrive/Coco Datasets/coco_ann2017.zip\" \".\"\n","!cp \"/content/drive/MyDrive/Coco Datasets/coco_val2017.zip\" \".\""],"metadata":{"id":"UjRIuPckEI99","executionInfo":{"status":"ok","timestamp":1717137995446,"user_tz":-420,"elapsed":27359,"user":{"displayName":"Norman Võ","userId":"13500557006273414443"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"-0r8w_OIHsex","executionInfo":{"status":"ok","timestamp":1717138110836,"user_tz":-420,"elapsed":5,"user":{"displayName":"Norman Võ","userId":"13500557006273414443"}}},"outputs":[],"source":["# extract zip files\n","def extract_zip_file(extract_path):\n","    try:\n","        with ZipFile(extract_path+\".zip\") as zfile:\n","            zfile.extractall(extract_path)\n","        # remove zipfile\n","        zfileTOremove=f\"{extract_path}\"+\".zip\"\n","        if os.path.isfile(zfileTOremove):\n","            os.remove(zfileTOremove)\n","        else:\n","            print(\"Error: %s file not found\" % zfileTOremove)\n","    except BadZipFile as e:\n","        print(\"Error:\", e)"]},{"cell_type":"code","source":["# extract_train_path = \"./coco_train2017\"\n","extract_val_path = \"./coco_val2017\"\n","extract_ann_path=\"./coco_ann2017\"\n","# extract_zip_file(extract_train_path)\n","extract_zip_file(extract_val_path)\n","extract_zip_file(extract_ann_path)"],"metadata":{"id":"8E10EiOODw8z","executionInfo":{"status":"ok","timestamp":1717138129405,"user_tz":-420,"elapsed":16129,"user":{"displayName":"Norman Võ","userId":"13500557006273414443"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"eRPja-HEHwg_","executionInfo":{"status":"ok","timestamp":1717138100294,"user_tz":-420,"elapsed":337,"user":{"displayName":"Norman Võ","userId":"13500557006273414443"}}},"outputs":[],"source":["dataDir='./coco_ann2017'\n","dataType='train2017'\n","annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Tq5AAluAK_Y","collapsed":true},"outputs":[],"source":["coco = COCO(annFile)"]},{"cell_type":"code","source":["# Display image using the given image and (optionally) annotation\n","\n","def show_coco_image(image, annotations = None):\n","  # Retrieve image dimensions\n","  height, width = image['height'], image['width']\n","\n","  # Create a new figure with the same dimensions as the image\n","  fig, ax = plt.subplots(figsize=(10,10), dpi=100)\n","\n","  I = io.imread(image['coco_url'])\n","  # Display the original image\n","  ax.imshow(I)\n","  ax.axis('off')\n","  ax.set_title(\"Image ID: {}\".format(image[\"id\"]))\n","\n","  # Draw bounding boxes on the original image\n","  if (annotations):\n","    for annotation in annotations:\n","        bbox = annotation['bbox']\n","        category_id = annotation['category_id']\n","        category_name = coco.loadCats(category_id)[0]['name']\n","\n","        # Convert COCO bounding box format (x, y, width, height) to matplotlib format (xmin, ymin, xmax, ymax)\n","        xmin, ymin, width, height = bbox\n","        xmax = xmin + width\n","        ymax = ymin + height\n","\n","        # Draw the bounding box rectangle\n","        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor='red', facecolor='none')\n","        ax.add_patch(rect)\n","\n","        # Add the category name as a label above the bounding box\n","        ax.text(xmin, ymin - 5, category_name, fontsize=8, color='red', weight='bold')\n","\n","# Show the plot\n","  plt.show()\n","# Clear plot after showing\n","  ax.clear()\n","  plt.clf()\n","  plt.cla()\n","  plt.close()\n","\n","def show_image(image):\n","\n","  # Create a new figure with the same dimensions as the image\n","  fig, ax = plt.subplots(figsize=(10,10), dpi=100)\n","\n","  # Display image\n","  ax.imshow(image)\n","  ax.axis('off')\n","\n","# Show the plot\n","  plt.show()\n","# Clear plot after showing\n","  ax.clear()\n","  plt.clf()\n","  plt.cla()\n","  plt.close()"],"metadata":{"id":"D5hQs_DSYI39","executionInfo":{"status":"ok","timestamp":1717137816369,"user_tz":-420,"elapsed":2,"user":{"displayName":"Norman Võ","userId":"13500557006273414443"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# helpers for acquiring annotations\n","\n","# return a list of annotations for a specific image id\n","'''\n","@param dict: a dictionary containing the annotations of images\n","@param image: an image we want to find annotations for\n","@return A list of annotations (bounding box + category)\n","'''\n","def get_annotations_for_image(dict, image):\n","  return list(dict[image[\"id\"]].values())[0]\n","\n","'''\n","@param data: an image set following Coco file format\n","@return a 2d dictionary where an image id is associated with a dictionary object that contains all annotations related to that id\n","'''\n","def create_label_dict(data):\n","  data_id = np.array([d[\"id\"] for d in data])\n","\n","  labelled_data = coco.loadAnns(coco.getAnnIds(imgIds = data_id))\n","  label_dict = {}\n","  for labels in labelled_data:\n","    image_id = labels['image_id']\n","\n","    if (image_id not in label_dict):\n","      label_dict[image_id] = {}\n","      label_dict_entry = label_dict[image_id]\n","      label_dict_entry[\"annotations\"] = []\n","\n","    label_dict_entry = label_dict[image_id]\n","    annotation_entry = {\"id\": labels[\"id\"], \"category_id\": labels[\"category_id\"], \"bbox\": labels[\"bbox\"]}\n","    label_dict_entry[\"annotations\"].append(annotation_entry)\n","\n","  return label_dict"],"metadata":{"id":"KK_Wbt6sQWR9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# General initialization\n","# use official cfg file\n","!mkdir cfg\n","!cd cfg\n","!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n","\n","# use official weight file\n","!wget https://pjreddie.com/media/files/yolov3.weights"],"metadata":{"id":"piI864Prn6Kf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# utilities\n","\n","\n","\n","def unique(tensor):\n","    tensor_np = tensor.cpu().numpy()\n","    unique_np = np.unique(tensor_np)\n","    unique_tensor = torch.from_numpy(unique_np)\n","\n","    tensor_res = tensor.new(unique_tensor.shape)\n","    tensor_res.copy_(unique_tensor)\n","    return tensor_res\n","\n","\n","def bbox_iou(box1, box2):\n","    \"\"\"\n","    Returns the IoU of two bounding boxes\n","\n","\n","    \"\"\"\n","    #Get the coordinates of bounding boxes\n","    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n","    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n","\n","    #get the corrdinates of the intersection rectangle\n","    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n","    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n","    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n","    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n","\n","    #Intersection area\n","    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n","\n","    #Union Area\n","    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n","    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n","\n","    iou = inter_area / (b1_area + b2_area - inter_area)\n","\n","    return iou\n","\n","def write_results(prediction, confidence, num_classes, nms_conf = 0.4):\n","    conf_mask = (prediction[:,:,4] > confidence).float().unsqueeze(2)\n","    prediction = prediction*conf_mask\n","\n","    box_corner = prediction.new(prediction.shape)\n","    box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n","    box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n","    box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2)\n","    box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n","    prediction[:,:,:4] = box_corner[:,:,:4]\n","\n","    batch_size = prediction.size(0)\n","\n","    write = False\n","\n","\n","\n","    for ind in range(batch_size):\n","        image_pred = prediction[ind]          #image Tensor\n","       #confidence threshholding\n","       #NMS\n","\n","        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n","        max_conf = max_conf.float().unsqueeze(1)\n","        max_conf_score = max_conf_score.float().unsqueeze(1)\n","        seq = (image_pred[:,:5], max_conf, max_conf_score)\n","        image_pred = torch.cat(seq, 1)\n","\n","        non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n","        try:\n","            image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n","        except:\n","            continue\n","\n","        if image_pred_.shape[0] == 0:\n","            continue\n","#\n","\n","        #Get the various classes detected in the image\n","        img_classes = unique(image_pred_[:,-1])  # -1 index holds the class index\n","\n","\n","        for cls in img_classes:\n","            #perform NMS\n","\n","\n","            #get the detections with one particular class\n","            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n","            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n","            image_pred_class = image_pred_[class_mask_ind].view(-1,7)\n","\n","            #sort the detections such that the entry with the maximum objectness\n","            #confidence is at the top\n","            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n","            image_pred_class = image_pred_class[conf_sort_index]\n","            idx = image_pred_class.size(0)   #Number of detections\n","\n","            for i in range(idx):\n","                #Get the IOUs of all boxes that come after the one we are looking at\n","                #in the loop\n","                try:\n","                    ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n","                except ValueError:\n","                    break\n","\n","                except IndexError:\n","                    break\n","\n","                #Zero out all the detections that have IoU > treshhold\n","                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n","                image_pred_class[i+1:] *= iou_mask\n","\n","                #Remove the non-zero entries\n","                non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n","                image_pred_class = image_pred_class[non_zero_ind].view(-1,7)\n","\n","            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)      #Repeat the batch_id for as many detections of the class cls in the image\n","            seq = batch_ind, image_pred_class\n","\n","            if not write:\n","                output = torch.cat(seq,1)\n","                write = True\n","            else:\n","                out = torch.cat(seq,1)\n","                output = torch.cat((output,out))\n","\n","    try:\n","        return output\n","    except:\n","        return 0\n","\n","def letterbox_image(img, inp_dim):\n","    '''resize image with unchanged aspect ratio using padding'''\n","    img_w, img_h = img.shape[1], img.shape[0]\n","    w, h = inp_dim\n","    new_w = int(img_w * min(w/img_w, h/img_h))\n","    new_h = int(img_h * min(w/img_w, h/img_h))\n","    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n","\n","    canvas = np.full((inp_dim[1], inp_dim[0], 3), 128)\n","\n","    canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image\n","\n","    return canvas\n","\n","def prep_image(img, inp_dim):\n","    \"\"\"\n","    Prepare image for inputting to the neural network.\n","\n","    Returns a Variable\n","    \"\"\"\n","    img = (letterbox_image(img, (inp_dim, inp_dim)))\n","    img = img[:,:,::-1].transpose((2,0,1)).copy()\n","    img = torch.from_numpy(img).float().div(255.0).unsqueeze(0)\n","    return img\n","\n","def load_classes(namesfile):\n","    fp = open(namesfile, \"r\")\n","    names = fp.read().split(\"\\n\")[:-1]\n","    return names"],"metadata":{"id":"_L5i80U2ueOL","executionInfo":{"status":"ok","timestamp":1717137828213,"user_tz":-420,"elapsed":334,"user":{"displayName":"Norman Võ","userId":"13500557006273414443"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def predict_transform(prediction, inp_dim, anchors, num_classes, CUDA = True):\n","\n","    batch_size = prediction.size(0)\n","    stride =  inp_dim // prediction.size(2)\n","    # grid_size = inp_dim // stride\n","    grid_size = prediction.size(2)\n","    bbox_attrs = 5 + num_classes\n","    num_anchors = len(anchors)\n","    # print(grid_size*grid_size)\n","    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n","    prediction = prediction.transpose(1,2).contiguous()\n","    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n","    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n","\n","    #Sigmoid the  centre_X, centre_Y. and object confidencce\n","    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n","    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n","    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n","\n","    #Add the center offsets\n","    grid = np.arange(grid_size)\n","    a,b = np.meshgrid(grid, grid)\n","\n","    x_offset = torch.FloatTensor(a).view(-1,1)\n","    y_offset = torch.FloatTensor(b).view(-1,1)\n","\n","    if CUDA:\n","        x_offset = x_offset.cuda()\n","        y_offset = y_offset.cuda()\n","\n","    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n","\n","    prediction[:,:,:2] += x_y_offset\n","\n","    #log space transform height and the width\n","    anchors = torch.FloatTensor(anchors)\n","\n","    if CUDA:\n","        anchors = anchors.cuda()\n","\n","    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n","    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n","\n","    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))\n","\n","    prediction[:,:,:4] *= stride\n","\n","    return prediction"],"metadata":{"id":"-mcHl5wtd4rX","executionInfo":{"status":"ok","timestamp":1717137836408,"user_tz":-420,"elapsed":344,"user":{"displayName":"Norman Võ","userId":"13500557006273414443"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Darknet-53\n","\n","def parse_cfg(cfgfile):\n","    \"\"\"\n","    Takes a configuration file\n","\n","    Returns a list of blocks. Each blocks describes a block in the neural\n","    network to be built. Block is represented as a dictionary in the list\n","\n","    \"\"\"\n","\n","    file = open(cfgfile, 'r')\n","    lines = file.read().split('\\n')                        # store the lines in a list\n","    lines = [x for x in lines if len(x) > 0]               # get read of the empty lines\n","    lines = [x for x in lines if x[0] != '#']              # get rid of comments\n","    lines = [x.rstrip().lstrip() for x in lines]           # get rid of fringe whitespaces\n","\n","    block = {}\n","    blocks = []\n","\n","    for line in lines:\n","        if line[0] == \"[\":               # This marks the start of a new block\n","            if len(block) != 0:          # If block is not empty, implies it is storing values of previous block.\n","                blocks.append(block)     # add it the blocks list\n","                block = {}               # re-init the block\n","            block[\"type\"] = line[1:-1].rstrip()\n","        else:\n","            key,value = line.split(\"=\")\n","            block[key.rstrip()] = value.lstrip()\n","    blocks.append(block)\n","\n","    return blocks\n","\n","\n","class EmptyLayer(nn.Module):\n","    def __init__(self):\n","        super(EmptyLayer, self).__init__()\n","\n","\n","class DetectionLayer(nn.Module):\n","    def __init__(self, anchors):\n","        super(DetectionLayer, self).__init__()\n","        self.anchors = anchors\n","\n","\n","\n","def create_modules(blocks):\n","    net_info = blocks[0]     #Captures the information about the input and pre-processing\n","    module_list = nn.ModuleList()\n","    prev_filters = 3\n","    output_filters = []\n","\n","    for index, x in enumerate(blocks[1:]):\n","        module = nn.Sequential()\n","\n","        #check the type of block\n","        #create a new module for the block\n","        #append to module_list\n","\n","        #If it's a convolutional layer\n","        if (x[\"type\"] == \"convolutional\"):\n","            #Get the info about the layer\n","            activation = x[\"activation\"]\n","            try:\n","                batch_normalize = int(x[\"batch_normalize\"])\n","                bias = False\n","            except:\n","                batch_normalize = 0\n","                bias = True\n","\n","            filters= int(x[\"filters\"])\n","            padding = int(x[\"pad\"])\n","            kernel_size = int(x[\"size\"])\n","            stride = int(x[\"stride\"])\n","\n","            if padding:\n","                pad = (kernel_size - 1) // 2\n","            else:\n","                pad = 0\n","\n","            #Add the convolutional layer\n","            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n","            module.add_module(\"conv_{0}\".format(index), conv)\n","\n","            #Add the Batch Norm Layer\n","            if batch_normalize:\n","                bn = nn.BatchNorm2d(filters)\n","                module.add_module(\"batch_norm_{0}\".format(index), bn)\n","\n","            #Check the activation.\n","            #It is either Linear or a Leaky ReLU for YOLO\n","            if activation == \"leaky\":\n","                activn = nn.LeakyReLU(0.1, inplace = True)\n","                module.add_module(\"leaky_{0}\".format(index), activn)\n","\n","            #If it's an upsampling layer\n","            #We use Bilinear2dUpsampling\n","        elif (x[\"type\"] == \"upsample\"):\n","            stride = int(x[\"stride\"])\n","            upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n","            module.add_module(\"upsample_{}\".format(index), upsample)\n","\n","        #If it is a route layer\n","        elif (x[\"type\"] == \"route\"):\n","            x[\"layers\"] = x[\"layers\"].split(',')\n","            #Start  of a route\n","            start = int(x[\"layers\"][0])\n","            #end, if there exists one.\n","            try:\n","                end = int(x[\"layers\"][1])\n","            except:\n","                end = 0\n","            #Positive anotation\n","            if start > 0:\n","                start = start - index\n","            if end > 0:\n","                end = end - index\n","            route = EmptyLayer()\n","            module.add_module(\"route_{0}\".format(index), route)\n","            if end < 0:\n","                filters = output_filters[index + start] + output_filters[index + end]\n","            else:\n","                filters= output_filters[index + start]\n","\n","        #shortcut corresponds to skip connection\n","        elif x[\"type\"] == \"shortcut\":\n","            shortcut = EmptyLayer()\n","            module.add_module(\"shortcut_{}\".format(index), shortcut)\n","\n","        #Yolo is the detection layer\n","        elif x[\"type\"] == \"yolo\":\n","            mask = x[\"mask\"].split(\",\")\n","            mask = [int(x) for x in mask]\n","\n","            anchors = x[\"anchors\"].split(\",\")\n","            anchors = [int(a) for a in anchors]\n","            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n","            anchors = [anchors[i] for i in mask]\n","\n","            detection = DetectionLayer(anchors)\n","            module.add_module(\"Detection_{}\".format(index), detection)\n","\n","        module_list.append(module)\n","        prev_filters = filters\n","        output_filters.append(filters)\n","\n","    return (net_info, module_list)\n","\n","class Darknet(nn.Module):\n","    def __init__(self, cfgfile):\n","        super(Darknet, self).__init__()\n","        self.blocks = parse_cfg(cfgfile)\n","        self.net_info, self.module_list = create_modules(self.blocks)\n","\n","    def forward(self, x, CUDA):\n","        modules = self.blocks[1:]\n","        outputs = {}   #We cache the outputs for the route layer\n","\n","        write = 0\n","        for i, module in enumerate(modules):\n","            module_type = (module[\"type\"])\n","\n","            if module_type == \"convolutional\" or module_type == \"upsample\":\n","                x = self.module_list[i](x)\n","\n","            elif module_type == \"route\":\n","                layers = module[\"layers\"]\n","                layers = [int(a) for a in layers]\n","\n","                if (layers[0]) > 0:\n","                    layers[0] = layers[0] - i\n","\n","                if len(layers) == 1:\n","                    x = outputs[i + (layers[0])]\n","\n","                else:\n","                    if (layers[1]) > 0:\n","                        layers[1] = layers[1] - i\n","\n","                    map1 = outputs[i + layers[0]]\n","                    map2 = outputs[i + layers[1]]\n","                    x = torch.cat((map1, map2), 1)\n","\n","\n","            elif  module_type == \"shortcut\":\n","                from_ = int(module[\"from\"])\n","                x = outputs[i-1] + outputs[i+from_]\n","\n","            elif module_type == 'yolo':\n","                anchors = self.module_list[i][0].anchors\n","                #Get the input dimensions\n","                inp_dim = int (self.net_info[\"height\"])\n","\n","                #Get the number of classes\n","                num_classes = int (module[\"classes\"])\n","\n","                #Transform\n","                x = x.data\n","                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n","                if not write:              #if no collector has been intialised.\n","                    detections = x\n","                    write = 1\n","\n","                else:\n","                    detections = torch.cat((detections, x), 1)\n","\n","            outputs[i] = x\n","\n","        return detections\n","\n","\n","    def load_weights(self, weightfile):\n","        #Open the weights file\n","        fp = open(weightfile, \"rb\")\n","\n","        #The first 5 values are header information\n","        # 1. Major version number\n","        # 2. Minor Version Number\n","        # 3. Subversion number\n","        # 4,5. Images seen by the network (during training)\n","        header = np.fromfile(fp, dtype = np.int32, count = 5)\n","        self.header = torch.from_numpy(header)\n","        self.seen = self.header[3]\n","\n","        weights = np.fromfile(fp, dtype = np.float32)\n","\n","        ptr = 0\n","        for i in range(len(self.module_list)):\n","            module_type = self.blocks[i + 1][\"type\"]\n","\n","            #If module_type is convolutional load weights\n","            #Otherwise ignore.\n","\n","            if module_type == \"convolutional\":\n","                model = self.module_list[i]\n","                try:\n","                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n","                except:\n","                    batch_normalize = 0\n","\n","                conv = model[0]\n","\n","\n","                if (batch_normalize):\n","                    bn = model[1]\n","\n","                    #Get the number of weights of Batch Norm Layer\n","                    num_bn_biases = bn.bias.numel()\n","\n","                    #Load the weights\n","                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n","                    ptr += num_bn_biases\n","\n","                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n","                    ptr  += num_bn_biases\n","\n","                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n","                    ptr  += num_bn_biases\n","\n","                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n","                    ptr  += num_bn_biases\n","\n","                    #Cast the loaded weights into dims of model weights.\n","                    bn_biases = bn_biases.view_as(bn.bias.data)\n","                    bn_weights = bn_weights.view_as(bn.weight.data)\n","                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n","                    bn_running_var = bn_running_var.view_as(bn.running_var)\n","\n","                    #Copy the data to model\n","                    bn.bias.data.copy_(bn_biases)\n","                    bn.weight.data.copy_(bn_weights)\n","                    bn.running_mean.copy_(bn_running_mean)\n","                    bn.running_var.copy_(bn_running_var)\n","\n","                else:\n","                    #Number of biases\n","                    num_biases = conv.bias.numel()\n","\n","                    #Load the weights\n","                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n","                    ptr = ptr + num_biases\n","\n","                    #reshape the loaded weights according to the dims of the model weights\n","                    conv_biases = conv_biases.view_as(conv.bias.data)\n","\n","                    #Finally copy the data\n","                    conv.bias.data.copy_(conv_biases)\n","\n","                #Let us load the weights for the Convolutional layers\n","                num_weights = conv.weight.numel()\n","\n","                #Do the same as above for weights\n","                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n","                ptr = ptr + num_weights\n","\n","                conv_weights = conv_weights.view_as(conv.weight.data)\n","                conv.weight.data.copy_(conv_weights)"],"metadata":{"id":"juQk0rHP6HMq","executionInfo":{"status":"ok","timestamp":1717137840159,"user_tz":-420,"elapsed":425,"user":{"displayName":"Norman Võ","userId":"13500557006273414443"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def evaluate(image_set):\n","  model = Darknet(\"cfg/yolov3.cfg\")\n","  model.load_weights(\"yolov3.weights\")\n","\n","  num_classes = len(coco.getCatIds())\n","\n","  classes = []\n","\n","  for data in coco.loadCats(coco.getCatIds()):\n","    classes.append(data[\"name\"])\n","\n","  batch_size = 3\n","  confidence = 0.5\n","  nms_thesh = 0.4\n","  start = 0\n","  CUDA = torch.cuda.is_available()\n","\n","  if CUDA:\n","    model.cuda()\n","\n","  inp_dim = int(model.net_info[\"height\"])\n","  assert inp_dim % 32 == 0\n","  assert inp_dim > 32\n","\n","  #PyTorch Variables for images\n","\n","  im_batches = list(map(prep_image, image_set, [inp_dim for x in range(len(image_set))]))\n","  im_dim_list = [(x.shape[1], x.shape[0]) for x in image_set]\n","  im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n","\n","  if CUDA:\n","      im_dim_list = im_dim_list.cuda()\n","\n","\n","  leftover = 0\n","  if (len(im_dim_list) % batch_size):\n","    leftover = 1\n","\n","  if batch_size != 1:\n","    num_batches = len(image_set) // batch_size + leftover\n","    im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,\n","                        len(im_batches))]))  for i in range(num_batches)]\n","\n","  write = 0\n","  start_det_loop = time.time()\n","  for i, batch in enumerate(im_batches):\n","      #load the image\n","      if CUDA:\n","          batch = batch.cuda()\n","\n","      prediction = model(Variable(batch, volatile = True), CUDA)\n","\n","      prediction = write_results(prediction, confidence, num_classes, nms_conf = nms_thesh)\n","\n","\n","      prediction[:,0] += i*batch_size    #transform the atribute from index in batch to index in imlist\n","\n","      if not write:                      #If we have't initialised output\n","          output = prediction\n","          write = 1\n","      else:\n","          output = torch.cat((output,prediction))\n","\n","      for im_num, image in enumerate(image_set[i*batch_size: min((i +  1)*batch_size, len(image_set))]):\n","          im_id = i*batch_size + im_num\n","          objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n","\n","      if CUDA:\n","          torch.cuda.synchronize()\n","\n","  try:\n","      output\n","  except NameError:\n","      print (\"No detections were made\")\n","      exit()\n","\n","  im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n","\n","  scaling_factor = torch.min(416/im_dim_list,1)[0].view(-1,1)\n","\n","\n","  output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n","  output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n","\n","\n","\n","  output[:,1:5] /= scaling_factor\n","\n","  for i in range(output.shape[0]):\n","      output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n","      output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n","\n","\n","  output_recast = time.time()\n","  class_load = time.time()\n","  # colors = pkl.load(open(\"pallete\", \"rb\"))\n","\n","  def write(x, results):\n","      c1 = tuple((int(x[1].item()), int(x[2].item())))\n","      c2 = tuple((int(x[3].item()), int(x[4].item())))\n","\n","      img = results[int(x[0])]\n","\n","      color = (int(np.random.randn() * 255), int(np.random.randn() * 255), int(np.random.randn() * 255))\n","      cls = int(x[-1])\n","      label = \"{0}\".format(classes[cls])\n","      cv2.rectangle(img, c1, c2,color, 1)\n","      t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n","      c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n","      cv2.rectangle(img, c1, c2, color, -1)\n","      cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n","      return img\n","\n","  draw = time.time()\n","  list(map(lambda x: write(x, image_set), output))\n","\n","  for img in image_set:\n","    show_image(img)\n","\n","  torch.cuda.empty_cache()\n"],"metadata":{"id":"NpCp-2dz8mxm","executionInfo":{"status":"ok","timestamp":1717142460339,"user_tz":-420,"elapsed":401,"user":{"displayName":"Norman Võ","userId":"13500557006273414443"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# load categories and images from annotated training sets;\n","imgIds = coco.getImgIds()\n","imgs = coco.loadImgs(imgIds)\n","\n","idx = int(np.random.randn() * 10000)\n","image_data = np.array(imgs[idx:idx+5])\n","\n","images = [io.imread(x[\"coco_url\"]) for x in image_data]\n","\n","evaluate(images)"],"metadata":{"id":"P0OAX0Uqypko"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1ZGsaTWfY-y4jaZPCuaEX7iHZ5RzyvWjb","authorship_tag":"ABX9TyPzGWQH26GZiMFbW255/xXg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}